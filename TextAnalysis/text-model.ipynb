{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0cf821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\shell\\envs\\FP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from json import loads,dumps\n",
    "import ast\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef, roc_curve, auc, average_precision_score\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import BeitForImageClassification, BeitFeatureExtractor, Trainer, TrainingArguments\n",
    "from transformers import FlavaConfig, FlavaModel, FlavaForPreTraining\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPFeatureExtractor\n",
    "from transformers import TrOCRProcessor, DebertaV2ForSequenceClassification, DebertaV2Config\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor\n",
    "from transformers import AutoProcessor, GitVisionModel, AutoModelForCausalLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import BertPreTrainedModel, RobertaConfig, RobertaTokenizerFast\n",
    "\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import RobertaClassificationHead, RobertaConfig, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c70fd594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544da7f",
   "metadata": {},
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ad671",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "caption_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "caption_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "caption_feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "caption_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84a514",
   "metadata": {},
   "source": [
    "max_length = 32\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = caption_feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = caption_model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = caption_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f129d",
   "metadata": {},
   "source": [
    "predict_step(['hm_data/img/01236.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931dcdcd",
   "metadata": {},
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
    "\n",
    "image = Image.open('hm_data/img/01236.png')\n",
    "\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef967cc2",
   "metadata": {},
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer_sent = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model_sent = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# #model.save_pretrained(MODEL)\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n",
    "# scores = output[0][0].detach().numpy()\n",
    "# scores = softmax(scores)\n",
    "# ranking = np.argsort(scores)\n",
    "# ranking = ranking[::-1]\n",
    "# for i in range(scores.shape[0]):\n",
    "#     l = config.id2label[ranking[i]]\n",
    "    \n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_sent, tokenizer=tokenizer_sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32157ae2",
   "metadata": {},
   "source": [
    "print(sentiment_task(\"a man with his head turned to the side of the road.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a302b42",
   "metadata": {},
   "source": [
    "img_dir = \"hm_data/img\"\n",
    "\n",
    "with open(\"train.jsonl\",encoding='utf8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "inc = 1\n",
    "\n",
    "for img in data:\n",
    "    img['text-sentiment'] = sentiment_task(img['text'])[0]['label']\n",
    "    image = Image.open('hm_data/'+img['img'])\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    img['caption'] = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    img['caption-sentiment'] = sentiment_task(img['caption'])[0]['label']\n",
    "    inc += 1\n",
    "    if inc%250 == 0:\n",
    "        print(inc)\n",
    "#     print(img)\n",
    "with open('train_sentiment.jsonl','w',encoding='utf8') as f:\n",
    "    for item in data:\n",
    "        json.dump(item,f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679e7d1",
   "metadata": {},
   "source": [
    "with open(\"train_dev_all.entity.jsonl\",encoding='utf8') as m:\n",
    "    datah = [json.loads(line) for line in m]\n",
    "\n",
    "dfdata = pd.DataFrame(data)\n",
    "dfdatah = pd.DataFrame(datah)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfb068",
   "metadata": {},
   "source": [
    "dfdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fd989",
   "metadata": {},
   "source": [
    "dfdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a67e08",
   "metadata": {},
   "source": [
    "dfdatah.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5c251",
   "metadata": {},
   "source": [
    "dfmerge = dfdata.merge(dfdatah[['img', 'partition_description']], how = 'left', on='img')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81cdc4",
   "metadata": {},
   "source": [
    "dfmerge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734c96b",
   "metadata": {},
   "source": [
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fae5f",
   "metadata": {},
   "source": [
    "# https://stackoverflow.com/questions/17864466/flatten-a-list-of-strings-and-lists-of-strings-and-lists-in-python\n",
    "def flatten_to_strings(listOfLists):\n",
    "    \"\"\"Flatten a list of (lists of (lists of strings)) for any level \n",
    "    of nesting\"\"\"\n",
    "    result = []\n",
    "\n",
    "    for i in listOfLists:\n",
    "        # Only append if i is a basestring (superclass of string)\n",
    "        if isinstance(i, str):\n",
    "            result.append(i)\n",
    "        # Otherwise call this function recursively\n",
    "        else:\n",
    "            result.extend(flatten_to_strings(i))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c311c",
   "metadata": {},
   "source": [
    "dfmerge['partition_description'] = dfmerge['partition_description'].apply(flatten_to_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48864d",
   "metadata": {},
   "source": [
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0567fd9",
   "metadata": {},
   "source": [
    "dfmerge['partition_description'] = \",\".join(dfmerge['partition_description'][0])\n",
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e60c11",
   "metadata": {},
   "source": [
    "dfmerge['texty'] = dfmerge['text'] + \"[SEP]\" + dfmerge['text-sentiment'] + \"[SEP]\" + dfmerge['caption'] + \"[SEP]\" + dfmerge['caption-sentiment'] + \"[SEP]\" + dfmerge['partition_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07ea38",
   "metadata": {},
   "source": [
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea67f16",
   "metadata": {},
   "source": [
    "outs = dfmerge[['id','img','label','texty']].to_dict('records')\n",
    "\n",
    "with open('train_text.jsonl', \"w\",encoding='utf8') as f:\n",
    "    for i in range(len(outs)):\n",
    "        f.write(str(outs[i])+\"\\n\")\n",
    "\n",
    "# print(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dede40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6f6816b",
   "metadata": {},
   "source": [
    "img_dir = \"hm_data/img\"\n",
    "\n",
    "with open(\"dev_seen.jsonl\",encoding='utf8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "inc = 1\n",
    "\n",
    "for img in data:\n",
    "    img['text-sentiment'] = sentiment_task(img['text'])[0]['label']\n",
    "    image = Image.open('hm_data/'+img['img'])\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    img['caption'] = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    img['caption-sentiment'] = sentiment_task(img['caption'])[0]['label']\n",
    "    inc += 1\n",
    "    if inc%250 == 0:\n",
    "        print(inc)\n",
    "#     print(img)\n",
    "with open('dev_seen_sentiment.jsonl','w',encoding='utf8') as f:\n",
    "    for item in data:\n",
    "        json.dump(item,f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0c4c7",
   "metadata": {},
   "source": [
    "dfdata = pd.DataFrame(data)\n",
    "dfmerge = dfdata.merge(dfdatah[['img', 'partition_description']], how = 'left', on='img')\n",
    "dfmerge['partition_description'] = dfmerge['partition_description'].apply(flatten_to_strings)\n",
    "dfmerge['partition_description'] = \",\".join(dfmerge['partition_description'][0])\n",
    "dfmerge['texty'] = dfmerge['text'] + \"[SEP]\" + dfmerge['text-sentiment'] + \"[SEP]\" + dfmerge['caption'] + \"[SEP]\" + dfmerge['caption-sentiment'] + \"[SEP]\" + dfmerge['partition_description']\n",
    "outs = dfmerge[['id','img','label','texty']].to_dict('records')\n",
    "\n",
    "with open('dev_seen_text.jsonl', \"w\",encoding='utf8') as f:\n",
    "    for i in range(len(outs)):\n",
    "        f.write(str(outs[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cb48e",
   "metadata": {},
   "source": [
    "img_dir = \"hm_data/img\"\n",
    "\n",
    "with open(\"dev_unseen.jsonl\",encoding='utf8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "inc = 1\n",
    "\n",
    "for img in data:\n",
    "    img['text-sentiment'] = sentiment_task(img['text'])[0]['label']\n",
    "    image = Image.open('hm_data/'+img['img'])\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    img['caption'] = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    img['caption-sentiment'] = sentiment_task(img['caption'])[0]['label']\n",
    "    inc += 1\n",
    "    if inc%250 == 0:\n",
    "        print(inc)\n",
    "#     print(img)\n",
    "with open('dev_unseen_sentiment.jsonl','w',encoding='utf8') as f:\n",
    "    for item in data:\n",
    "        json.dump(item,f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd2a13",
   "metadata": {},
   "source": [
    "dfdata = pd.DataFrame(data)\n",
    "dfmerge = dfdata.merge(dfdatah[['img', 'partition_description']], how = 'left', on='img')\n",
    "dfmerge['partition_description'] = dfmerge['partition_description'].apply(flatten_to_strings)\n",
    "dfmerge['partition_description'] = \",\".join(dfmerge['partition_description'][0])\n",
    "dfmerge['texty'] = dfmerge['text'] + \"[SEP]\" + dfmerge['text-sentiment'] + \"[SEP]\" + dfmerge['caption'] + \"[SEP]\" + dfmerge['caption-sentiment'] + \"[SEP]\" + dfmerge['partition_description']\n",
    "outs = dfmerge[['id','img','label','texty']].to_dict('records')\n",
    "\n",
    "with open('dev_unseen_text.jsonl', \"w\",encoding='utf8') as f:\n",
    "    for i in range(len(outs)):\n",
    "        f.write(str(outs[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2f9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pchanda/pchanda.github.io/blob/master/_posts/2021-04-15-Roberta-FineTuning-for-Classification.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788af8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675725c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>texty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet[SEP]neutral[SEP]a gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                               texty  \n",
       "0  its their character not their color that matte...  \n",
       "1  don't be afraid to love again everyone is not ...  \n",
       "2  putting bows on your pet[SEP]neutral[SEP]a gra...  \n",
       "3  i love everything and everybody! except for sq...  \n",
       "4  everybody loves chocolate chip cookies, even h...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"train_text.jsonl\", \"r\", encoding='utf8') as f:\n",
    "    train_df = pd.DataFrame([ast.literal_eval(line) for line in f.readlines()])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7b88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def county(st):\n",
    "    return st.count(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384837a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_df['texty'].apply(county))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398fc7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>texty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range[SEP]nega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46971</td>\n",
       "      <td>img/46971.png</td>\n",
       "      <td>1</td>\n",
       "      <td>bravery at its finest[SEP]negative[SEP]a man w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03745</td>\n",
       "      <td>img/03745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>your order comes to $37.50 and your white priv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83745</td>\n",
       "      <td>img/83745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>it is time.. to send these parasites back to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80243</td>\n",
       "      <td>img/80243.png</td>\n",
       "      <td>1</td>\n",
       "      <td>mississippi wind chime[SEP]neutral[SEP]a poste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  08291  img/08291.png      1   \n",
       "1  46971  img/46971.png      1   \n",
       "2  03745  img/03745.png      1   \n",
       "3  83745  img/83745.png      1   \n",
       "4  80243  img/80243.png      1   \n",
       "\n",
       "                                               texty  \n",
       "0  white people is this a shooting range[SEP]nega...  \n",
       "1  bravery at its finest[SEP]negative[SEP]a man w...  \n",
       "2  your order comes to $37.50 and your white priv...  \n",
       "3  it is time.. to send these parasites back to t...  \n",
       "4  mississippi wind chime[SEP]neutral[SEP]a poste...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"dev_seen_text.jsonl\", \"r\", encoding='utf8') as f:\n",
    "    test_df = pd.DataFrame([ast.literal_eval(line) for line in f.readlines()])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d80c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'pchanda/pretrained-smiles-pubchem10m'\n",
    "model_name = \"microsoft/deberta-large\"\n",
    "num_labels = 2\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer_name = model_name\n",
    "\n",
    "max_seq_length = 100 \n",
    "train_batch_size = 8\n",
    "test_batch_size = 8\n",
    "warmup_ratio = 0.06\n",
    "weight_decay= 2e-05#1e-04 # from .0\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 25\n",
    "learning_rate = 1e-05 \n",
    "adam_epsilon = 1e-05#1e-04 # from 1e-08\n",
    "\n",
    "num_hidden_layers = 12\n",
    "num_attention_heads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e00d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaForSmilesClassification(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(RobertaForSmilesClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a7c41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deberta to instantiate a model of type deberta-v2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaV2ForSequenceClassification: ['deberta.encoder.layer.14.attention.self.pos_proj.weight', 'deberta.encoder.layer.13.attention.self.pos_proj.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.16.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.14.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.weight', 'lm_predictions.lm_head.dense.weight', 'deberta.encoder.layer.7.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.20.attention.self.in_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.21.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.14.attention.self.pos_q_proj.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.21.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.19.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.12.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.13.attention.self.q_bias', 'lm_predictions.lm_head.bias', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.19.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.20.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.13.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.15.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.11.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.23.attention.self.in_proj.weight', 'deberta.encoder.layer.20.attention.self.v_bias', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.8.attention.self.v_bias', 'deberta.encoder.layer.13.attention.self.v_bias', 'deberta.encoder.layer.15.attention.self.v_bias', 'deberta.encoder.layer.13.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.21.attention.self.q_bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.20.attention.self.pos_proj.weight', 'deberta.encoder.layer.16.attention.self.v_bias', 'deberta.encoder.layer.22.attention.self.v_bias', 'deberta.encoder.layer.22.attention.self.pos_proj.weight', 'deberta.encoder.layer.22.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.9.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.18.attention.self.q_bias', 'deberta.encoder.layer.15.attention.self.in_proj.weight', 'deberta.encoder.layer.16.attention.self.in_proj.weight', 'deberta.encoder.layer.23.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.19.attention.self.pos_proj.weight', 'deberta.encoder.layer.23.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.20.attention.self.q_bias', 'deberta.encoder.layer.17.attention.self.pos_proj.weight', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.17.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.15.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.12.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.16.attention.self.q_bias', 'deberta.encoder.layer.9.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.11.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.12.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.22.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.15.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.18.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.22.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.21.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.14.attention.self.v_bias', 'deberta.encoder.layer.18.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.12.attention.self.v_bias', 'deberta.encoder.layer.21.attention.self.pos_proj.weight', 'deberta.encoder.layer.13.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.20.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.17.attention.self.v_bias', 'deberta.encoder.layer.12.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.21.attention.self.in_proj.weight', 'deberta.encoder.layer.16.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.22.attention.self.in_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.17.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.23.attention.self.v_bias', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.8.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.15.attention.self.pos_proj.weight', 'deberta.encoder.layer.16.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.19.attention.self.in_proj.weight', 'deberta.encoder.layer.17.attention.self.q_bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'lm_predictions.lm_head.dense.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.14.attention.self.in_proj.weight', 'deberta.encoder.layer.12.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.23.attention.self.pos_proj.weight', 'deberta.encoder.layer.23.attention.self.q_bias', 'deberta.encoder.layer.18.attention.self.in_proj.weight', 'deberta.encoder.layer.19.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.18.attention.self.pos_proj.weight', 'deberta.encoder.layer.14.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.pos_proj.weight', 'deberta.encoder.layer.17.attention.self.in_proj.weight', 'deberta.encoder.layer.18.attention.self.v_bias', 'deberta.encoder.layer.19.attention.self.v_bias', 'deberta.encoder.layer.7.attention.self.pos_proj.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['deberta.encoder.layer.5.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.14.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.6.attention.self.value_proj.weight', 'deberta.encoder.layer.16.attention.self.query_proj.weight', 'deberta.encoder.layer.13.attention.self.value_proj.weight', 'deberta.encoder.layer.5.attention.self.key_proj.bias', 'deberta.encoder.layer.20.attention.self.query_proj.weight', 'deberta.encoder.layer.10.attention.self.value_proj.bias', 'deberta.encoder.layer.2.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.0.attention.self.key_proj.bias', 'deberta.encoder.layer.14.attention.self.value_proj.weight', 'deberta.encoder.layer.11.attention.self.query_proj.weight', 'deberta.encoder.layer.21.attention.self.query_proj.weight', 'deberta.encoder.layer.13.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.14.attention.self.key_proj.bias', 'deberta.encoder.layer.17.attention.self.key_proj.bias', 'deberta.encoder.layer.7.attention.self.key_proj.weight', 'deberta.encoder.layer.19.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.3.attention.self.key_proj.weight', 'deberta.encoder.layer.0.attention.self.key_proj.weight', 'deberta.encoder.layer.8.attention.self.query_proj.weight', 'deberta.encoder.layer.4.attention.self.value_proj.bias', 'deberta.encoder.layer.13.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.20.attention.self.key_proj.bias', 'deberta.encoder.layer.4.attention.self.key_proj.bias', 'deberta.encoder.layer.5.attention.self.key_proj.weight', 'deberta.encoder.layer.14.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.10.attention.self.key_proj.weight', 'deberta.encoder.layer.7.attention.self.query_proj.bias', 'deberta.encoder.layer.1.attention.self.value_proj.weight', 'deberta.encoder.layer.7.attention.self.value_proj.weight', 'deberta.encoder.layer.15.attention.self.query_proj.bias', 'deberta.encoder.layer.12.attention.self.query_proj.bias', 'deberta.encoder.layer.23.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.1.attention.self.key_proj.weight', 'deberta.encoder.layer.6.attention.self.key_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.22.attention.self.key_proj.weight', 'deberta.encoder.layer.18.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.9.attention.self.query_proj.bias', 'deberta.encoder.layer.21.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.16.attention.self.query_proj.bias', 'deberta.encoder.layer.14.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.12.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.16.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.5.attention.self.pos_query_proj.bias', 'pooler.dense.bias', 'deberta.encoder.layer.14.attention.self.query_proj.weight', 'deberta.encoder.layer.9.attention.self.key_proj.bias', 'deberta.encoder.layer.21.attention.self.query_proj.bias', 'deberta.encoder.layer.8.attention.self.query_proj.bias', 'deberta.encoder.layer.23.attention.self.query_proj.bias', 'deberta.encoder.layer.3.attention.self.value_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.14.attention.self.query_proj.bias', 'deberta.encoder.layer.2.attention.self.key_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.22.attention.self.key_proj.bias', 'deberta.encoder.layer.19.attention.self.query_proj.weight', 'deberta.encoder.layer.11.attention.self.query_proj.bias', 'deberta.encoder.layer.3.attention.self.query_proj.weight', 'deberta.encoder.layer.6.attention.self.query_proj.weight', 'deberta.encoder.layer.22.attention.self.value_proj.weight', 'deberta.encoder.layer.18.attention.self.key_proj.weight', 'deberta.encoder.layer.19.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.13.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.16.attention.self.value_proj.weight', 'deberta.encoder.layer.13.attention.self.key_proj.weight', 'deberta.encoder.layer.2.attention.self.value_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.12.attention.self.value_proj.bias', 'deberta.encoder.layer.9.attention.self.value_proj.weight', 'deberta.encoder.layer.17.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.18.attention.self.query_proj.weight', 'deberta.encoder.layer.5.attention.self.query_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.17.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.22.attention.self.query_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.15.attention.self.key_proj.weight', 'deberta.encoder.layer.10.attention.self.query_proj.bias', 'deberta.encoder.layer.6.attention.self.value_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.16.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.21.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.15.attention.self.query_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.12.attention.self.key_proj.bias', 'deberta.encoder.layer.9.attention.self.query_proj.weight', 'deberta.encoder.layer.0.attention.self.query_proj.weight', 'deberta.encoder.layer.17.attention.self.query_proj.weight', 'deberta.encoder.layer.17.attention.self.key_proj.weight', 'deberta.encoder.layer.16.attention.self.key_proj.weight', 'deberta.encoder.layer.19.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.8.attention.self.key_proj.weight', 'deberta.encoder.layer.18.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.13.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.18.attention.self.key_proj.bias', 'deberta.encoder.layer.11.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.7.attention.self.value_proj.bias', 'deberta.encoder.layer.21.attention.self.key_proj.weight', 'deberta.encoder.layer.12.attention.self.value_proj.weight', 'classifier.bias', 'deberta.encoder.layer.19.attention.self.query_proj.bias', 'deberta.encoder.layer.18.attention.self.query_proj.bias', 'deberta.encoder.layer.22.attention.self.query_proj.bias', 'deberta.encoder.layer.13.attention.self.query_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.15.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.15.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.9.attention.self.key_proj.weight', 'deberta.encoder.layer.11.attention.self.value_proj.bias', 'deberta.encoder.layer.21.attention.self.value_proj.bias', 'deberta.encoder.layer.18.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.11.attention.self.key_proj.bias', 'deberta.encoder.layer.15.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.23.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.12.attention.self.key_proj.weight', 'deberta.encoder.layer.14.attention.self.key_proj.weight', 'deberta.encoder.layer.15.attention.self.value_proj.weight', 'deberta.encoder.layer.18.attention.self.value_proj.weight', 'deberta.encoder.layer.20.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.4.attention.self.value_proj.weight', 'deberta.encoder.layer.5.attention.self.value_proj.bias', 'deberta.encoder.layer.17.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.22.attention.self.value_proj.bias', 'deberta.encoder.layer.23.attention.self.key_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.20.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.12.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.20.attention.self.query_proj.bias', 'deberta.encoder.layer.15.attention.self.key_proj.bias', 'deberta.encoder.layer.5.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.6.attention.self.query_proj.bias', 'deberta.encoder.layer.2.attention.self.value_proj.bias', 'deberta.encoder.layer.20.attention.self.key_proj.weight', 'deberta.encoder.layer.3.attention.self.key_proj.bias', 'deberta.encoder.layer.8.attention.self.value_proj.weight', 'deberta.encoder.layer.0.attention.self.value_proj.weight', 'deberta.encoder.layer.1.attention.self.value_proj.bias', 'deberta.encoder.layer.7.attention.self.query_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.21.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.18.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.11.attention.self.value_proj.weight', 'deberta.encoder.layer.11.attention.self.key_proj.weight', 'deberta.encoder.layer.5.attention.self.value_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.10.attention.self.value_proj.weight', 'deberta.encoder.layer.12.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.16.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.19.attention.self.value_proj.weight', 'deberta.encoder.layer.23.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.23.attention.self.key_proj.bias', 'deberta.encoder.layer.2.attention.self.query_proj.bias', 'deberta.encoder.layer.14.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.14.attention.self.value_proj.bias', 'deberta.encoder.layer.2.attention.self.key_proj.bias', 'deberta.encoder.layer.16.attention.self.key_proj.bias', 'deberta.encoder.layer.17.attention.self.value_proj.bias', 'deberta.encoder.layer.15.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.15.attention.self.value_proj.bias', 'deberta.encoder.layer.23.attention.self.value_proj.weight', 'pooler.dense.weight', 'deberta.encoder.layer.23.attention.self.query_proj.weight', 'deberta.encoder.layer.13.attention.self.query_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.4.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.16.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.13.attention.self.key_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.19.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.1.attention.self.query_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.19.attention.self.value_proj.bias', 'deberta.encoder.layer.12.attention.self.query_proj.weight', 'deberta.encoder.layer.1.attention.self.key_proj.bias', 'deberta.encoder.layer.3.attention.self.value_proj.weight', 'deberta.encoder.layer.16.attention.self.value_proj.bias', 'deberta.encoder.layer.20.attention.self.value_proj.weight', 'deberta.encoder.layer.20.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.0.attention.self.value_proj.bias', 'deberta.encoder.layer.1.attention.self.query_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.12.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.21.attention.self.value_proj.weight', 'deberta.encoder.layer.23.attention.self.value_proj.bias', 'deberta.encoder.layer.8.attention.self.value_proj.bias', 'deberta.encoder.layer.3.attention.self.query_proj.bias', 'deberta.encoder.layer.22.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.20.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.2.attention.self.query_proj.weight', 'deberta.encoder.layer.5.attention.self.query_proj.bias', 'deberta.encoder.layer.0.attention.self.query_proj.bias', 'deberta.encoder.layer.10.attention.self.query_proj.weight', 'deberta.encoder.layer.19.attention.self.key_proj.bias', 'deberta.encoder.layer.22.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.22.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.7.attention.self.key_proj.bias', 'deberta.encoder.layer.17.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.2.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.4.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_query_proj.weight', 'classifier.weight', 'deberta.encoder.layer.21.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.17.attention.self.query_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.8.attention.self.key_proj.bias', 'deberta.encoder.layer.4.attention.self.query_proj.bias', 'deberta.encoder.layer.10.attention.self.key_proj.bias', 'deberta.encoder.layer.18.attention.self.value_proj.bias', 'deberta.encoder.layer.6.attention.self.key_proj.bias', 'deberta.encoder.layer.17.attention.self.value_proj.weight', 'deberta.encoder.layer.20.attention.self.value_proj.bias', 'deberta.encoder.layer.13.attention.self.value_proj.bias', 'deberta.encoder.layer.22.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.9.attention.self.value_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.4.attention.self.query_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.23.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.19.attention.self.key_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.4.attention.self.key_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.21.attention.self.key_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_query_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=\n",
      " DebertaV2ForSequenceClassification(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (pos_key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (pos_query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(1024, 1024)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (dropout): StableDropout()\n",
      ") \n",
      "\n",
      "Tokenizer= DebertaTokenizerFast(name_or_path='microsoft/deberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_class = DebertaV2Config\n",
    "# model_class = RobertaForSmilesClassification\n",
    "model_class = DebertaV2ForSequenceClassification\n",
    "\n",
    "# tokenizer_class = RobertaTokenizerFast\n",
    "tokenizer_class = AutoTokenizer\n",
    "\n",
    "config = config_class.from_pretrained(model_name, num_labels=num_labels)#, num_hidden_layers = num_hidden_layers,num_attention_heads=num_attention_heads)\n",
    "\n",
    "model = model_class.from_pretrained(model_name, config=config)\n",
    "print('Model=\\n',model,'\\n')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "print('Tokenizer=',tokenizer,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4beb0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer):\n",
    "        text, labels = data\n",
    "        self.examples = tokenizer(text=text,text_pair=None,truncation=True,padding=\"max_length\",\n",
    "                                  max_length=max_seq_length,return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n",
    "\n",
    "\n",
    "train_examples = (train_df.iloc[:, 3].astype(str).replace('[SEP]',' [SEP] ').tolist(), train_df.iloc[:, 2].tolist())\n",
    "train_dataset = MyClassificationDataset(train_examples,tokenizer)\n",
    "\n",
    "test_examples = (test_df.iloc[:, 3].astype(str).replace('[SEP]',' [SEP] ').tolist(), test_df.iloc[:, 2].tolist())\n",
    "test_dataset = MyClassificationDataset(test_examples,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3827e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  2527,    47,  4161,     7, 11619,   930,    53,    47,   214,\n",
      "         22169,    30,    22, 29126,    24,    18,  2569,   751,   113,     2,\n",
      "         33407,     2,   102,   313,  1826,    10, 18896,    11,   760,     9,\n",
      "            10, 18896,     4,     2, 12516,     2,   597, 27015,  2549,     6,\n",
      "          3628,     6, 43457,  2943, 24876,     6,   397,     2,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,  8310,    89,    18,    10,  2173,     2, 12516,     2,   102,\n",
      "          9843,  1580,     9,  2356,     9,  8676,  1311,    10,  1901,     4,\n",
      "             2, 12516,     2,   597, 27015,  2549,     6,  3628,     6, 43457,\n",
      "          2943, 24876,     6,   397,     2,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,  8338,    80,   383,   283,    31, 11721, 10839,   749, 32310,\n",
      "          4755,     8, 24791, 26536,   268,   328,    61,    32,    47,   475,\n",
      "          2678, 21639,   116,     2, 33407,     2,  5970,     6,    10,  9716,\n",
      "             6,    16,     5,   129,   621,     7,    33,    10,   865,    15,\n",
      "            39,   471,     4,     2, 12516,     2,   597, 27015,  2549,     6,\n",
      "          3628,     6, 43457,  2943, 24876,     6,   397,     2,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1, 14746,   110, 19323,  4136,    41,    47,   214,    10,  2292,\n",
      "         11599,  6906, 12846,     2, 33407,     2,   627, 19495,    16,    41,\n",
      "           505,   233,     9,     5,   284,     4,     2, 22173,     2,   597,\n",
      "         27015,  2549,     6,  3628,     6, 43457,  2943, 24876,     6,   397,\n",
      "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1, 13839,     9,    82,  8262,    31,  5679, 33307, 14119,   811,\n",
      "          3342,    30,    24, 21999,  3673,  2510,     2, 12516,     2,   102,\n",
      "         11255,  3627,    16,  7973,    19,  6738, 12668,     4,     2, 12516,\n",
      "             2,   597, 27015,  2549,     6,  3628,     6, 43457,  2943, 24876,\n",
      "             6,   397,     2,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1, 22466,  2297, 11619,  2642,  1719,     2, 12516,     2,   102,\n",
      "           920,   223,   514,    19,   930,     2, 12516,     2,   597, 27015,\n",
      "          2549,     6,  3628,     6, 43457,  2943, 24876,     6,   397,     2,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1, 13792, 10839,    29,  9930, 40873, 14917,    29,     2, 33407,\n",
      "             2, 17522, 14320,    16,     5,   235,     7,   235,     2, 12516,\n",
      "             2,   597, 27015,  2549,     6,  3628,     6, 43457,  2943, 24876,\n",
      "             6,   397,     2,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   281,   811,  2377,  1296,   439,    25,   421,     2, 22173,\n",
      "             2,  5970,    21,     5,  1393,     9,     5,   512,     2, 12516,\n",
      "             2,   597, 27015,  2549,     6,  3628,     6, 43457,  2943, 24876,\n",
      "             6,   397,     2,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]], device='cuda:0'), 'labels': tensor([0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "def get_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(device)\n",
    "    return inputs\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=train_batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "#Extract a batch as sanity-check\n",
    "batch = get_inputs_dict(next(iter(train_dataloader)))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a201546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\shell\\envs\\FP\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf4e7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, model_outputs, labels, eval_examples=None, multi_label=False):\n",
    "    assert len(preds) == len(labels)\n",
    "    mismatched = labels != preds\n",
    "    wrong = [i for (i, v) in zip(eval_examples, mismatched) if v.any()]\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n",
    "    scores = np.array([softmax(element)[1] for element in model_outputs])\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    auprc = average_precision_score(labels, scores)\n",
    "    return (\n",
    "        {\n",
    "            **{\"mcc\": mcc, \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn, \"auroc\": auroc, \"auprc\": auprc},\n",
    "        },\n",
    "        wrong,\n",
    "    )\n",
    "\n",
    "def print_confusion_matrix(result):\n",
    "    print('confusion matrix:')\n",
    "    print('            predicted    ')\n",
    "    print('          0     |     1')\n",
    "    print('    ----------------------')\n",
    "    print('   0 | ',format(result['tn'],'5d'),' | ',format(result['fp'],'5d'))\n",
    "    print('gt -----------------------')\n",
    "    print('   1 | ',format(result['fn'],'5d'),' | ',format(result['tp'],'5d'))\n",
    "    print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f56a57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Training avg loss 0.6545800213839575\n",
      "epoch 0 Testing  avg loss 0.7509521860924978\n",
      "{'mcc': 0.0, 'tp': 0, 'tn': 253, 'fp': 0, 'fn': 247, 'auroc': 0.5011441647597255, 'auprc': 0.4924028614603228}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    253  |      0\n",
      "gt -----------------------\n",
      "   1 |    247  |      0\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 1 Training avg loss 0.6558560478384498\n",
      "epoch 1 Testing  avg loss 0.7349526063790397\n",
      "{'mcc': 0.0, 'tp': 0, 'tn': 253, 'fp': 0, 'fn': 247, 'auroc': 0.5087132547086781, 'auprc': 0.5098741806397499}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    253  |      0\n",
      "gt -----------------------\n",
      "   1 |    247  |      0\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 2 Training avg loss 0.6535120451102701\n",
      "epoch 2 Testing  avg loss 0.7121043072806464\n",
      "{'mcc': 0.0, 'tp': 0, 'tn': 253, 'fp': 0, 'fn': 247, 'auroc': 0.5110495911411244, 'auprc': 0.5090238663117459}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    253  |      0\n",
      "gt -----------------------\n",
      "   1 |    247  |      0\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = get_inputs_dict(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    #evaluate model with test_df at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(test_dataloader)\n",
    "    preds = np.empty((len(test_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(test_dataset)))\n",
    "    model.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = get_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result, wrong = compute_metrics(preds, model_outputs, out_label_ids, test_examples)\n",
    "    \n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    print(result)\n",
    "    print_confusion_matrix(result)\n",
    "    print('---------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec28bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b630dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
