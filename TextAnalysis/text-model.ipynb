{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb0cf821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import loads,dumps\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef, roc_curve, auc, average_precision_score\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import BeitForImageClassification, BeitFeatureExtractor, Trainer, TrainingArguments\n",
    "from transformers import FlavaConfig, FlavaModel, FlavaForPreTraining\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPFeatureExtractor\n",
    "from transformers import DebertaForSequenceClassification, DebertaConfig\n",
    "from transformers import TrOCRProcessor, DebertaV2ForSequenceClassification, DebertaV2Config\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor\n",
    "from transformers import AutoProcessor, GitVisionModel, AutoModelForCausalLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import BertPreTrainedModel, RobertaConfig, RobertaTokenizerFast\n",
    "\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import RobertaClassificationHead, RobertaConfig, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c70fd594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe42c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "caption_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "caption_feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "caption_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b71a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = caption_feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = caption_model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = caption_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248dc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_step(['hm_data/img/01236.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179300a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
    "\n",
    "image = Image.open('hm_data/img/01236.png')\n",
    "\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f64185",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer_sent = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model_sent = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_sent, tokenizer=tokenizer_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21341dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_task(\"a man with his head turned to the side of the road.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fa4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"hm_data/img\"\n",
    "\n",
    "with open(\"train.jsonl\",encoding='utf8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "inc = 1\n",
    "\n",
    "for img in data:\n",
    "    img['text-sentiment'] = sentiment_task(img['text'])[0]['label']\n",
    "    image = Image.open('hm_data/'+img['img'])\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    img['caption'] = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    img['caption-sentiment'] = sentiment_task(img['caption'])[0]['label']\n",
    "    inc += 1\n",
    "    if inc%250 == 0:\n",
    "        print(inc)\n",
    "#     print(img)\n",
    "with open('train_sentiment.jsonl','w',encoding='utf8') as f:\n",
    "    for item in data:\n",
    "        json.dump(item,f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_dev_all.entity.jsonl\",encoding='utf8') as m:\n",
    "    datah = [json.loads(line) for line in m]\n",
    "\n",
    "dfdata = pd.DataFrame(data)\n",
    "dfdatah = pd.DataFrame(datah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52489d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f582f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7422e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdatah.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge = dfdata.merge(dfdatah[['img', 'partition_description']], how = 'left', on='img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ce2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc324986",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a576c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/17864466/flatten-a-list-of-strings-and-lists-of-strings-and-lists-in-python\n",
    "def flatten_to_strings(listOfLists):\n",
    "    \"\"\"Flatten a list of (lists of (lists of strings)) for any level \n",
    "    of nesting\"\"\"\n",
    "    result = []\n",
    "\n",
    "    for i in listOfLists:\n",
    "        # Only append if i is a basestring (superclass of string)\n",
    "        if isinstance(i, str):\n",
    "            result.append(i)\n",
    "        # Otherwise call this function recursively\n",
    "        else:\n",
    "            result.extend(flatten_to_strings(i))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge['partition_description'] = dfmerge['partition_description'].apply(flatten_to_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge['partition_description'] = \",\".join(dfmerge['partition_description'][0])\n",
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7426513",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge['texty'] = dfmerge['text'] + \"[SEP]\" + dfmerge['text-sentiment'] + \"[SEP]\" + dfmerge['caption'] + \"[SEP]\" + dfmerge['caption-sentiment'] + \"[SEP]\" + dfmerge['partition_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1504935",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = dfmerge[['id','img','label','texty']].to_dict('records')\n",
    "\n",
    "with open('train_text.jsonl', \"w\",encoding='utf8') as f:\n",
    "    for i in range(len(outs)):\n",
    "        f.write(str(outs[i])+\"\\n\")\n",
    "\n",
    "# print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45243d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70854da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"hm_data/img\"\n",
    "\n",
    "with open(\"dev_seen.jsonl\",encoding='utf8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "inc = 1\n",
    "\n",
    "for img in data:\n",
    "    img['text-sentiment'] = sentiment_task(img['text'])[0]['label']\n",
    "    image = Image.open('hm_data/'+img['img'])\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    img['caption'] = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    img['caption-sentiment'] = sentiment_task(img['caption'])[0]['label']\n",
    "    inc += 1\n",
    "    if inc%250 == 0:\n",
    "        print(inc)\n",
    "#     print(img)\n",
    "with open('dev_seen_sentiment.jsonl','w',encoding='utf8') as f:\n",
    "    for item in data:\n",
    "        json.dump(item,f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = pd.DataFrame(data)\n",
    "dfmerge = dfdata.merge(dfdatah[['img', 'partition_description']], how = 'left', on='img')\n",
    "dfmerge['partition_description'] = dfmerge['partition_description'].apply(flatten_to_strings)\n",
    "dfmerge['partition_description'] = \",\".join(dfmerge['partition_description'][0])\n",
    "dfmerge['texty'] = dfmerge['text'] + \"[SEP]\" + dfmerge['text-sentiment'] + \"[SEP]\" + dfmerge['caption'] + \"[SEP]\" + dfmerge['caption-sentiment'] + \"[SEP]\" + dfmerge['partition_description']\n",
    "outs = dfmerge[['id','img','label','texty']].to_dict('records')\n",
    "\n",
    "with open('dev_seen_text.jsonl', \"w\",encoding='utf8') as f:\n",
    "    for i in range(len(outs)):\n",
    "        f.write(str(outs[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52102dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"hm_data/img\"\n",
    "\n",
    "with open(\"dev_unseen.jsonl\",encoding='utf8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "inc = 1\n",
    "\n",
    "for img in data:\n",
    "    img['text-sentiment'] = sentiment_task(img['text'])[0]['label']\n",
    "    image = Image.open('hm_data/'+img['img'])\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    img['caption'] = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    img['caption-sentiment'] = sentiment_task(img['caption'])[0]['label']\n",
    "    inc += 1\n",
    "    if inc%250 == 0:\n",
    "        print(inc)\n",
    "#     print(img)\n",
    "with open('dev_unseen_sentiment.jsonl','w',encoding='utf8') as f:\n",
    "    for item in data:\n",
    "        json.dump(item,f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = pd.DataFrame(data)\n",
    "dfmerge = dfdata.merge(dfdatah[['img', 'partition_description']], how = 'left', on='img')\n",
    "dfmerge['partition_description'] = dfmerge['partition_description'].apply(flatten_to_strings)\n",
    "dfmerge['partition_description'] = \",\".join(dfmerge['partition_description'][0])\n",
    "dfmerge['texty'] = dfmerge['text'] + \"[SEP]\" + dfmerge['text-sentiment'] + \"[SEP]\" + dfmerge['caption'] + \"[SEP]\" + dfmerge['caption-sentiment'] + \"[SEP]\" + dfmerge['partition_description']\n",
    "outs = dfmerge[['id','img','label','texty']].to_dict('records')\n",
    "\n",
    "with open('dev_unseen_text.jsonl', \"w\",encoding='utf8') as f:\n",
    "    for i in range(len(outs)):\n",
    "        f.write(str(outs[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2f9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of fine tuning referenced: https://github.com/pchanda/pchanda.github.io/blob/master/_posts/2021-04-15-Roberta-FineTuning-for-Classification.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "675725c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>texty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet[SEP]neutral[SEP]a gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                               texty  \n",
       "0  its their character not their color that matte...  \n",
       "1  don't be afraid to love again everyone is not ...  \n",
       "2  putting bows on your pet[SEP]neutral[SEP]a gra...  \n",
       "3  i love everything and everybody! except for sq...  \n",
       "4  everybody loves chocolate chip cookies, even h...  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"train_text.jsonl\", \"r\", encoding='utf8') as f:\n",
    "    train_df = pd.DataFrame([ast.literal_eval(line) for line in f.readlines()])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8d7b88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def county(st):\n",
    "    return st.count(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "384837a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_df['texty'].apply(county))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "40638e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, 4)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "24a0db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resplit(s):\n",
    "    ns = s.split('[SEP]')\n",
    "    while len(ns)<5:\n",
    "        ns.append('*')\n",
    "#     dns = pd.DataFrame(ns).T.reindex(range(5), axis=1).fillna('')\n",
    "    dns = pd.Series(ns)\n",
    "#     print(ns)\n",
    "    return dns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6bdc6ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>texty</th>\n",
       "      <th>text</th>\n",
       "      <th>tsent</th>\n",
       "      <th>caption</th>\n",
       "      <th>csent</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matte...</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a man with a bald head and a black suit.</td>\n",
       "      <td>negative</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a bride and groom are not going to love.</td>\n",
       "      <td>negative</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet[SEP]neutral[SEP]a gra...</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a gray cat with a red bow</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>positive</td>\n",
       "      <td>person is a black and brown dog with brown spots.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a man in a suit and tie is dancing with cookies.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                               texty  \\\n",
       "0  its their character not their color that matte...   \n",
       "1  don't be afraid to love again everyone is not ...   \n",
       "2  putting bows on your pet[SEP]neutral[SEP]a gra...   \n",
       "3  i love everything and everybody! except for sq...   \n",
       "4  everybody loves chocolate chip cookies, even h...   \n",
       "\n",
       "                                                text     tsent  \\\n",
       "0   its their character not their color that matters   neutral   \n",
       "1  don't be afraid to love again everyone is not ...   neutral   \n",
       "2                           putting bows on your pet   neutral   \n",
       "3  i love everything and everybody! except for sq...  positive   \n",
       "4  everybody loves chocolate chip cookies, even h...  positive   \n",
       "\n",
       "                                             caption     csent  \\\n",
       "0           a man with a bald head and a black suit.  negative   \n",
       "1           a bride and groom are not going to love.  negative   \n",
       "2                          a gray cat with a red bow   neutral   \n",
       "3  person is a black and brown dog with brown spots.   neutral   \n",
       "4   a man in a suit and tie is dancing with cookies.   neutral   \n",
       "\n",
       "                                   extra  \n",
       "0  Facial hair,head,models male bald,man  \n",
       "1  Facial hair,head,models male bald,man  \n",
       "2  Facial hair,head,models male bald,man  \n",
       "3  Facial hair,head,models male bald,man  \n",
       "4  Facial hair,head,models male bald,man  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['text','tsent','caption','csent','extra']] = train_df['texty'].apply(resplit)#.reindex(range(5), axis=1).fillna('')#.str.split('[SEP]',expand=True).reindex(range(5), axis=1).fillna('')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "33de7b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>texty</th>\n",
       "      <th>text</th>\n",
       "      <th>tsent</th>\n",
       "      <th>caption</th>\n",
       "      <th>csent</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matte...</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a man with a bald head and a black suit.</td>\n",
       "      <td>negative</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a bride and groom are not going to love.</td>\n",
       "      <td>negative</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet [SEP] neutral [SEP] F...</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a gray cat with a red bow</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>positive</td>\n",
       "      <td>person is a black and brown dog with brown spots.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a man in a suit and tie is dancing with cookies.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Facial hair,head,models male bald,man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                               texty  \\\n",
       "0  its their character not their color that matte...   \n",
       "1  don't be afraid to love again everyone is not ...   \n",
       "2  putting bows on your pet [SEP] neutral [SEP] F...   \n",
       "3  i love everything and everybody! except for sq...   \n",
       "4  everybody loves chocolate chip cookies, even h...   \n",
       "\n",
       "                                                text     tsent  \\\n",
       "0   its their character not their color that matters   neutral   \n",
       "1  don't be afraid to love again everyone is not ...   neutral   \n",
       "2                           putting bows on your pet   neutral   \n",
       "3  i love everything and everybody! except for sq...  positive   \n",
       "4  everybody loves chocolate chip cookies, even h...  positive   \n",
       "\n",
       "                                             caption     csent  \\\n",
       "0           a man with a bald head and a black suit.  negative   \n",
       "1           a bride and groom are not going to love.  negative   \n",
       "2                          a gray cat with a red bow   neutral   \n",
       "3  person is a black and brown dog with brown spots.   neutral   \n",
       "4   a man in a suit and tie is dancing with cookies.   neutral   \n",
       "\n",
       "                                   extra  \n",
       "0  Facial hair,head,models male bald,man  \n",
       "1  Facial hair,head,models male bald,man  \n",
       "2  Facial hair,head,models male bald,man  \n",
       "3  Facial hair,head,models male bald,man  \n",
       "4  Facial hair,head,models male bald,man  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for no-caption experiment\n",
    "train_df['texty'] = train_df[['text','tsent','extra']].apply(lambda x: ' [SEP] '.join(x.values.tolist()), axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "398fc7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>texty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range[SEP]nega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46971</td>\n",
       "      <td>img/46971.png</td>\n",
       "      <td>1</td>\n",
       "      <td>bravery at its finest[SEP]negative[SEP]a man w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03745</td>\n",
       "      <td>img/03745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>your order comes to $37.50 and your white priv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83745</td>\n",
       "      <td>img/83745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>it is time.. to send these parasites back to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80243</td>\n",
       "      <td>img/80243.png</td>\n",
       "      <td>1</td>\n",
       "      <td>mississippi wind chime[SEP]neutral[SEP]a poste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  08291  img/08291.png      1   \n",
       "1  46971  img/46971.png      1   \n",
       "2  03745  img/03745.png      1   \n",
       "3  83745  img/83745.png      1   \n",
       "4  80243  img/80243.png      1   \n",
       "\n",
       "                                               texty  \n",
       "0  white people is this a shooting range[SEP]nega...  \n",
       "1  bravery at its finest[SEP]negative[SEP]a man w...  \n",
       "2  your order comes to $37.50 and your white priv...  \n",
       "3  it is time.. to send these parasites back to t...  \n",
       "4  mississippi wind chime[SEP]neutral[SEP]a poste...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use test_seenunseen_text.jsonl for final test\n",
    "with open(\"dev_seenunseen_text.jsonl\", \"r\", encoding='utf8') as f:\n",
    "    test_df = pd.DataFrame([ast.literal_eval(line) for line in f.readlines()])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e26dda51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>texty</th>\n",
       "      <th>text</th>\n",
       "      <th>tsent</th>\n",
       "      <th>caption</th>\n",
       "      <th>csent</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range [SEP] ne...</td>\n",
       "      <td>white people is this a shooting range</td>\n",
       "      <td>negative</td>\n",
       "      <td>people that are people in a car</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Peanut allergy,Family,Parent,Health,Gender ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46971</td>\n",
       "      <td>img/46971.png</td>\n",
       "      <td>1</td>\n",
       "      <td>bravery at its finest [SEP] negative [SEP] Pea...</td>\n",
       "      <td>bravery at its finest</td>\n",
       "      <td>negative</td>\n",
       "      <td>a man with a beard and glasses eating a spoon ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Peanut allergy,Family,Parent,Health,Gender ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03745</td>\n",
       "      <td>img/03745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>your order comes to $37.50 and your white priv...</td>\n",
       "      <td>your order comes to $37.50 and your white priv...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>person, a professional chef, is selling her ne...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Peanut allergy,Family,Parent,Health,Gender ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83745</td>\n",
       "      <td>img/83745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>it is time.. to send these parasites back to t...</td>\n",
       "      <td>it is time.. to send these parasites back to t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>a knight in a full armor</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Peanut allergy,Family,Parent,Health,Gender ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80243</td>\n",
       "      <td>img/80243.png</td>\n",
       "      <td>1</td>\n",
       "      <td>mississippi wind chime [SEP] neutral [SEP] Pea...</td>\n",
       "      <td>mississippi wind chime</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a poster for the documentary</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Peanut allergy,Family,Parent,Health,Gender ide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  08291  img/08291.png      1   \n",
       "1  46971  img/46971.png      1   \n",
       "2  03745  img/03745.png      1   \n",
       "3  83745  img/83745.png      1   \n",
       "4  80243  img/80243.png      1   \n",
       "\n",
       "                                               texty  \\\n",
       "0  white people is this a shooting range [SEP] ne...   \n",
       "1  bravery at its finest [SEP] negative [SEP] Pea...   \n",
       "2  your order comes to $37.50 and your white priv...   \n",
       "3  it is time.. to send these parasites back to t...   \n",
       "4  mississippi wind chime [SEP] neutral [SEP] Pea...   \n",
       "\n",
       "                                                text     tsent  \\\n",
       "0              white people is this a shooting range  negative   \n",
       "1                              bravery at its finest  negative   \n",
       "2  your order comes to $37.50 and your white priv...   neutral   \n",
       "3  it is time.. to send these parasites back to t...  negative   \n",
       "4                             mississippi wind chime   neutral   \n",
       "\n",
       "                                             caption    csent  \\\n",
       "0                    people that are people in a car  neutral   \n",
       "1  a man with a beard and glasses eating a spoon ...  neutral   \n",
       "2  person, a professional chef, is selling her ne...  neutral   \n",
       "3                           a knight in a full armor  neutral   \n",
       "4                       a poster for the documentary  neutral   \n",
       "\n",
       "                                               extra  \n",
       "0  Peanut allergy,Family,Parent,Health,Gender ide...  \n",
       "1  Peanut allergy,Family,Parent,Health,Gender ide...  \n",
       "2  Peanut allergy,Family,Parent,Health,Gender ide...  \n",
       "3  Peanut allergy,Family,Parent,Health,Gender ide...  \n",
       "4  Peanut allergy,Family,Parent,Health,Gender ide...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for no caption experiment\n",
    "test_df[['text','tsent','caption','csent','extra']] = test_df['texty'].apply(resplit)\n",
    "test_df.head()\n",
    "test_df['texty'] = test_df[['text','tsent','extra']].apply(lambda x: ' [SEP] '.join(x.values.tolist()), axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "30d80c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-large\"\n",
    "num_labels = 2\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer_name = model_name\n",
    "\n",
    "max_seq_length = 100 \n",
    "train_batch_size = 24\n",
    "test_batch_size = 24\n",
    "warmup_ratio = 0.06\n",
    "weight_decay= 6.3e-05#1e-04 # from .0\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 20\n",
    "learning_rate = 4.8e-07 \n",
    "adam_epsilon = 1e-05#1e-04 # from 1e-08\n",
    "\n",
    "num_hidden_layers = 12\n",
    "num_attention_heads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9a7c41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=\n",
      " DebertaForSequenceClassification(\n",
      "  (deberta): DebertaModel(\n",
      "    (embeddings): DebertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n",
      "      (LayerNorm): DebertaLayerNorm()\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x DebertaLayer(\n",
      "          (attention): DebertaAttention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): DebertaLayerNorm()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): DebertaLayerNorm()\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(1024, 1024)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (dropout): StableDropout()\n",
      ") \n",
      "\n",
      "Tokenizer= DebertaTokenizerFast(name_or_path='microsoft/deberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_class = DebertaConfig\n",
    "model_class = DebertaForSequenceClassification\n",
    "\n",
    "tokenizer_class = AutoTokenizer\n",
    "\n",
    "config = config_class.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "model = model_class.from_pretrained(model_name, config=config)\n",
    "print('Model=\\n',model,'\\n')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "print('Tokenizer=',tokenizer,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4beb0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer):\n",
    "        text, labels = data\n",
    "        self.examples = tokenizer(text=text,text_pair=None,truncation=True,padding=\"max_length\",\n",
    "                                  max_length=max_seq_length,return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n",
    "\n",
    "\n",
    "train_examples = (train_df.iloc[:, 3].astype(str).replace('[SEP]',' [SEP] ').tolist(), train_df.iloc[:, 2].tolist())\n",
    "train_dataset = MyClassificationDataset(train_examples,tokenizer)\n",
    "\n",
    "test_examples = (test_df.iloc[:, 3].astype(str).replace('[SEP]',' [SEP] ').tolist(), test_df.iloc[:, 2].tolist())\n",
    "test_dataset = MyClassificationDataset(test_examples,tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3827e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  4656,  7506,  ...,     0,     0,     0],\n",
      "        [    1,  1794,  2828,  ...,     0,     0,     0],\n",
      "        [    1,   627,  1457,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1,  2977,    28,  ...,     0,     0,     0],\n",
      "        [    1, 14746,    47,  ...,     0,     0,     0],\n",
      "        [    1,  9178,    47,  ...,     0,     0,     0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "def get_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(device)\n",
    "    return inputs\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=train_batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "#Extract a batch as sanity-check\n",
    "batch = get_inputs_dict(next(iter(train_dataloader)))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0a201546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\shell\\envs\\FP\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bf4e7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, model_outputs, labels, eval_examples=None, multi_label=False):\n",
    "    assert len(preds) == len(labels)\n",
    "    mismatched = labels != preds\n",
    "    wrong = [i for (i, v) in zip(eval_examples, mismatched) if v.any()]\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n",
    "    scores = np.array([softmax(element)[1] for element in model_outputs])\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    auprc = average_precision_score(labels, scores)\n",
    "    return (\n",
    "        {\n",
    "            **{\"mcc\": mcc, \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn, \"auroc\": auroc, \"auprc\": auprc},\n",
    "        },\n",
    "        wrong,\n",
    "    )\n",
    "\n",
    "def print_confusion_matrix(result):\n",
    "    print('confusion matrix:')\n",
    "    print('            predicted    ')\n",
    "    print('          0     |     1')\n",
    "    print('    ----------------------')\n",
    "    print('   0 | ',format(result['tn'],'5d'),' | ',format(result['fp'],'5d'))\n",
    "    print('gt -----------------------')\n",
    "    print('   1 | ',format(result['fn'],'5d'),' | ',format(result['tp'],'5d'))\n",
    "    print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "36f56a57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Training avg loss 0.6731157044289817\n",
      "epoch 0 Testing  avg loss 0.6872977512803945\n",
      "{'mcc': 0.0, 'tp': 0, 'tn': 593, 'fp': 0, 'fn': 447, 'auroc': 0.48192371100573056, 'auprc': 0.41998784119818866}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    593  |      0\n",
      "gt -----------------------\n",
      "   1 |    447  |      0\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 1 Training avg loss 0.6224236540391411\n",
      "epoch 1 Testing  avg loss 0.6771883869713003\n",
      "{'mcc': 0.102666584413626, 'tp': 36, 'tn': 573, 'fp': 20, 'fn': 411, 'auroc': 0.5903361740816611, 'auprc': 0.5160740138380092}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    573  |     20\n",
      "gt -----------------------\n",
      "   1 |    411  |     36\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 2 Training avg loss 0.5771160801531563\n",
      "epoch 2 Testing  avg loss 0.7074843184514479\n",
      "{'mcc': 0.10611121285362642, 'tp': 71, 'tn': 540, 'fp': 53, 'fn': 376, 'auroc': 0.6039664844513357, 'auprc': 0.5257965740758057}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    540  |     53\n",
      "gt -----------------------\n",
      "   1 |    376  |     71\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 3 Training avg loss 0.5468741184389088\n",
      "epoch 3 Testing  avg loss 0.7202267910946499\n",
      "{'mcc': 0.10952240438558641, 'tp': 92, 'tn': 519, 'fp': 74, 'fn': 355, 'auroc': 0.6142429764100938, 'auprc': 0.5325094421550298}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    519  |     74\n",
      "gt -----------------------\n",
      "   1 |    355  |     92\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 4 Training avg loss 0.5194730733901682\n",
      "epoch 4 Testing  avg loss 0.737413304773244\n",
      "{'mcc': 0.12760359834133014, 'tp': 91, 'tn': 527, 'fp': 66, 'fn': 356, 'auroc': 0.6207959376921655, 'auprc': 0.5361196135530564}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    527  |     66\n",
      "gt -----------------------\n",
      "   1 |    356  |     91\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 5 Training avg loss 0.4989300585128892\n",
      "epoch 5 Testing  avg loss 0.7166620377789844\n",
      "{'mcc': 0.13323295847572425, 'tp': 124, 'tn': 494, 'fp': 99, 'fn': 323, 'auroc': 0.6264359360322329, 'auprc': 0.5414441655442834}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    494  |     99\n",
      "gt -----------------------\n",
      "   1 |    323  |    124\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 6 Training avg loss 0.48286548735390245\n",
      "epoch 6 Testing  avg loss 0.7347321185198697\n",
      "{'mcc': 0.13386579298288234, 'tp': 101, 'tn': 519, 'fp': 74, 'fn': 346, 'auroc': 0.6293860890101143, 'auprc': 0.5430864219939111}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    519  |     74\n",
      "gt -----------------------\n",
      "   1 |    346  |    101\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 7 Training avg loss 0.47052347819570084\n",
      "epoch 7 Testing  avg loss 0.7403143691745672\n",
      "{'mcc': 0.1207976726954615, 'tp': 98, 'tn': 517, 'fp': 76, 'fn': 349, 'auroc': 0.6284014471594404, 'auprc': 0.5400061993003211}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    517  |     76\n",
      "gt -----------------------\n",
      "   1 |    349  |     98\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 8 Training avg loss 0.45759972413660777\n",
      "epoch 8 Testing  avg loss 0.7358946671540086\n",
      "{'mcc': 0.1270761677466055, 'tp': 117, 'tn': 499, 'fp': 94, 'fn': 330, 'auroc': 0.63045750006602, 'auprc': 0.5427677826660797}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    499  |     94\n",
      "gt -----------------------\n",
      "   1 |    330  |    117\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 9 Training avg loss 0.4479329902521321\n",
      "epoch 9 Testing  avg loss 0.7724405168132349\n",
      "{'mcc': 0.1134948455713599, 'tp': 85, 'tn': 528, 'fp': 65, 'fn': 362, 'auroc': 0.6269452335412022, 'auprc': 0.537734847470362}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    528  |     65\n",
      "gt -----------------------\n",
      "   1 |    362  |     85\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 10 Training avg loss 0.4423430667796605\n",
      "epoch 10 Testing  avg loss 0.7328811728141524\n",
      "{'mcc': 0.13027950813759462, 'tp': 130, 'tn': 486, 'fp': 107, 'fn': 317, 'auroc': 0.6295784902912804, 'auprc': 0.5399881820919821}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    486  |    107\n",
      "gt -----------------------\n",
      "   1 |    317  |    130\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 11 Training avg loss 0.43588390123676246\n",
      "epoch 11 Testing  avg loss 0.757146647030657\n",
      "{'mcc': 0.12253510113825862, 'tp': 107, 'tn': 508, 'fp': 85, 'fn': 340, 'auroc': 0.6288541560563018, 'auprc': 0.541030875136052}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    508  |     85\n",
      "gt -----------------------\n",
      "   1 |    340  |    107\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 12 Training avg loss 0.4293340758538582\n",
      "epoch 12 Testing  avg loss 0.7663817446340214\n",
      "{'mcc': 0.11928464504728713, 'tp': 103, 'tn': 511, 'fp': 82, 'fn': 344, 'auroc': 0.6284165374560022, 'auprc': 0.5402559447264774}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    511  |     82\n",
      "gt -----------------------\n",
      "   1 |    344  |    103\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 13 Training avg loss 0.4263936543968362\n",
      "epoch 13 Testing  avg loss 0.7519676272164691\n",
      "{'mcc': 0.1412942930104971, 'tp': 130, 'tn': 491, 'fp': 102, 'fn': 317, 'auroc': 0.6295747177171399, 'auprc': 0.5425159185031161}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    491  |    102\n",
      "gt -----------------------\n",
      "   1 |    317  |    130\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 14 Training avg loss 0.416767006273001\n",
      "epoch 14 Testing  avg loss 0.767083923925053\n",
      "{'mcc': 0.12407589999696264, 'tp': 114, 'tn': 501, 'fp': 92, 'fn': 333, 'auroc': 0.628318450528349, 'auprc': 0.5404085442627116}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    501  |     92\n",
      "gt -----------------------\n",
      "   1 |    333  |    114\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 15 Training avg loss 0.4136662705683372\n",
      "epoch 15 Testing  avg loss 0.7748784659938379\n",
      "{'mcc': 0.12575554021655502, 'tp': 111, 'tn': 505, 'fp': 88, 'fn': 336, 'auroc': 0.628171320136869, 'auprc': 0.5401728265011065}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    505  |     88\n",
      "gt -----------------------\n",
      "   1 |    336  |    111\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 16 Training avg loss 0.4087855405370954\n",
      "epoch 16 Testing  avg loss 0.7819278186017816\n",
      "{'mcc': 0.12128296725742194, 'tp': 112, 'tn': 502, 'fp': 91, 'fn': 335, 'auroc': 0.6277563369814125, 'auprc': 0.540592223008947}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    502  |     91\n",
      "gt -----------------------\n",
      "   1 |    335  |    112\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 Training avg loss 0.40796115662850124\n",
      "epoch 17 Testing  avg loss 0.7839373668486421\n",
      "{'mcc': 0.13466731850121552, 'tp': 107, 'tn': 513, 'fp': 80, 'fn': 340, 'auroc': 0.6274884842174361, 'auprc': 0.541375626050528}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    513  |     80\n",
      "gt -----------------------\n",
      "   1 |    340  |    107\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 18 Training avg loss 0.4087685693317736\n",
      "epoch 18 Testing  avg loss 0.7778351144357161\n",
      "{'mcc': 0.12684854501017648, 'tp': 116, 'tn': 500, 'fp': 93, 'fn': 331, 'auroc': 0.6284089923077214, 'auprc': 0.5414981743495166}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    500  |     93\n",
      "gt -----------------------\n",
      "   1 |    331  |    116\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 19 Training avg loss 0.4060969481585731\n",
      "epoch 19 Testing  avg loss 0.7839438427578319\n",
      "{'mcc': 0.13069327426817343, 'tp': 112, 'tn': 506, 'fp': 87, 'fn': 335, 'auroc': 0.6277940627228177, 'auprc': 0.540653769772447}\n",
      "confusion matrix:\n",
      "            predicted    \n",
      "          0     |     1\n",
      "    ----------------------\n",
      "   0 |    506  |     87\n",
      "gt -----------------------\n",
      "   1 |    335  |    112\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = get_inputs_dict(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    #evaluate model with test_df at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(test_dataloader)\n",
    "    preds = np.empty((len(test_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(test_dataset)))\n",
    "    model.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = get_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result, wrong = compute_metrics(preds, model_outputs, out_label_ids, test_examples)\n",
    "    \n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    print(result)\n",
    "    print_confusion_matrix(result)\n",
    "    print('---------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec28bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b630dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [0.678385621,0.632376498,0.592045852,0.556179642,0.530158635,0.506936796,0.492930673,0.47866862,0.466178885,0.456818814,0.44298748,0.43295857,0.424309122,0.412495723,0.405447978,0.398407371,0.389174767,0.390648836,0.381018993,0.377208777]\n",
    "validation_loss = [0.693061956,0.675622712,0.70049012,0.706153017,0.699088515,0.722747884,0.703253298,0.73466689,0.713488277,0.707019135,0.746260223,0.738967306,0.753794444,0.783485738,0.769607162,0.778863441,0.788041065,0.796995666,0.797464624,0.79262726]\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(range(20),train_loss, label='Training Loss')\n",
    "plt.plot(range(20),validation_loss, label='Validation Loss')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlim(left=0)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(f\"TextAnalysis/text_loss_vs_epoch.png\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0ba9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
